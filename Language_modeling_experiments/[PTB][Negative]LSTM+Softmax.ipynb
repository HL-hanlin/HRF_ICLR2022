{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[PTB][Negative]LSTM+Softmax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBXFtMqqJHqP",
        "outputId": "fd4408a4-0e2d-4611-d686-0b200bfe579d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import torch\n",
        "os.chdir('/content/drive/MyDrive/Softmax_sampling/code')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGDl3pbTOs7X"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lQCS4XKIf77"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from copy import deepcopy\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from data_utils import *\n",
        "#import model\n",
        "import easydict\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "#import Adam\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxOt0VSyOuzK"
      },
      "source": [
        "def gram_schmidt_columns(X):\n",
        "    '''\n",
        "    Using QR decomoposition to obtain orthogonal matrix.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : matrix, dimension = m * d, where m <= d\n",
        "        Random feature matrix with l2 normalized row.\n",
        "    Returns\n",
        "    -------\n",
        "    Q : matrix, dimension = m * d, where m <= d\n",
        "        Orthogonal random feature matrix with l2 normalized row.\n",
        "    '''\n",
        "    Q, R = np.linalg.qr(X)\n",
        "    return Q\n",
        "\n",
        "def orthgonalize(V):\n",
        "    '''\n",
        "    Generate matrix with multiple orthogonal blocks\n",
        "    Parameters\n",
        "    ----------\n",
        "    V : matrix, dimension = m * d, where m > d\n",
        "        Random feature matrix with l2 normalized row.\n",
        "    Returns\n",
        "    -------\n",
        "    V_ : TYPE\n",
        "        Random feature matrix with l2 normalized row and multiple\n",
        "        blocks.\n",
        "    '''\n",
        "    N = V.shape[0]\n",
        "    d = V.shape[1]\n",
        "    turns = int(N/d)\n",
        "    remainder = N%d\n",
        "\n",
        "    if turns:\n",
        "        V_ = np.zeros_like(V)\n",
        "\n",
        "        for i in range(turns):\n",
        "            v = gram_schmidt_columns(V[i*d:(i+1)*d, :].T).T\n",
        "            V_[i*d:(i+1)*d, :] = v\n",
        "        if remainder != 0:\n",
        "            V_[(i+1)*d:,:] = gram_schmidt_columns(V[(i+1)*d:,:].T).T\n",
        "    else:\n",
        "        V_ = gram_schmidt_columns(V.T).T\n",
        "\n",
        "    return V_\n",
        "\n",
        "def orthogonal_gau(dim_0, dim_1):\n",
        "\n",
        "    V = np.random.normal(0, 1, (dim_0, dim_1))\n",
        "    norms = np.linalg.norm(V, axis = 1)[:, np.newaxis]\n",
        "    V_orth = orthgonalize(V)\n",
        "    \n",
        "    return V_orth*norms\n",
        "\n",
        "def trig_att(x, y, random_feats_sfm, normalize=False):\n",
        "    \n",
        "    l, d = x.shape\n",
        "  \n",
        "    normalizer = 1 / (d ** 0.25) if normalize else 1\n",
        "    \n",
        "    x = x * normalizer\n",
        "    y = y * normalizer\n",
        "    \n",
        "    x_feat = np.sqrt(1/(random_feats_sfm.shape[0])) *\\\n",
        "                 np.exp(np.linalg.norm(x, axis = 1)**2/2)[:, np.newaxis] *\\\n",
        "                 np.vstack((np.sin(random_feats_sfm.dot(x.T)), \\\n",
        "                            np.cos(random_feats_sfm.dot(x.T)))).T\n",
        "    #print('x_feat shape ', x_feat.shape)  \n",
        "    y_feat = np.sqrt(1/(random_feats_sfm.shape[0])) *\\\n",
        "                 np.exp(np.linalg.norm(y, axis = 1)**2/2)[:, np.newaxis] *\\\n",
        "                 np.vstack((np.sin(random_feats_sfm.dot(y.T)), \\\n",
        "                            np.cos(random_feats_sfm.dot(y.T)))).T\n",
        "    #print('y_feat shape ', y_feat.shape)    \n",
        "  \n",
        "    return np.dot(x_feat, y_feat.T)\n",
        "    \n",
        "def pos_att(x, y, random_feats_sfm, normalize=False):\n",
        "    \n",
        "    l, d = x.shape\n",
        "  \n",
        "    normalizer = 1 / (d ** 0.25) if normalize else 1\n",
        "    \n",
        "    x = x * normalizer\n",
        "   \n",
        "    \n",
        "    x_feat = np.sqrt(1/(2*random_feats_sfm.shape[0])) * \\\n",
        "                    np.exp(-np.linalg.norm(x, axis = 1)**2/2)[:, np.newaxis] *\\\n",
        "                    np.vstack((np.exp(random_feats_sfm.dot(x.T)), \\\n",
        "                                np.exp(-random_feats_sfm.dot(x.T)))).T\n",
        "    del x\n",
        "    #print('x_feat shape ', x_feat.shape)  \n",
        "    y = y * normalizer\n",
        "    y_feat = np.sqrt(1/(2*random_feats_sfm.shape[0])) * \\\n",
        "                    np.exp(-np.linalg.norm(y, axis = 1)**2/2)[:, np.newaxis] *\\\n",
        "                    np.vstack((np.exp(random_feats_sfm.dot(y.T)), \\\n",
        "                                np.exp(-random_feats_sfm.dot(y.T)))).T\n",
        "    #print('y_feat shape ', y_feat.shape)    \n",
        "    \n",
        "    del y\n",
        "    return np.dot(x_feat, y_feat.T)\n",
        "\n",
        "def ang_hyb_lambda(x, y, random_feats_lambda, normalize=False):\n",
        "    \n",
        "    l, d = x.shape\n",
        "  \n",
        "    normalizer = 1 / (d ** 0.25) if normalize else 1\n",
        "    \n",
        "    x = x * normalizer\n",
        "    \n",
        "    x_feat = np.hstack((np.repeat(np.sqrt(1/2), x.shape[0])[:, np.newaxis],\\\n",
        "                                      (1j*np.sqrt(1/(2*random_feats_lambda.shape[0])) *\\\n",
        "                                      np.sign(random_feats_lambda.dot(x.T))).T))\n",
        "    #print('x_feat shape ', x_feat.shape)  \n",
        "    del x \n",
        "    \n",
        "    y = y * normalizer\n",
        "    y_feat = np.hstack((np.repeat(np.sqrt(1/2), y.shape[0])[:, np.newaxis],\\\n",
        "                                      (1j*np.sqrt(1/(2*random_feats_lambda.shape[0])) *\\\n",
        "                                      np.sign(random_feats_lambda.dot(y.T))).T))\n",
        "    #print('y_feat shape ', y_feat.shape) \n",
        "    del y\n",
        "  \n",
        "    return np.dot(x_feat, y_feat.T).real\n",
        "\n",
        "def gau_hyb_lambda(x, y, random_feats_lambda, lambda_=1, normalize=False):\n",
        "    \n",
        "    l, d = x.shape\n",
        "  \n",
        "    normalizer = 1 / (d ** 0.25) if normalize else 1\n",
        "    \n",
        "    x = x * normalizer\n",
        "\n",
        "    x_feat = (1*np.sqrt(1/(random_feats_lambda.shape[0])) *\\\n",
        "                      np.vstack((np.sin(lambda_*random_feats_lambda.dot(x.T)), \\\n",
        "                                np.cos(lambda_*random_feats_lambda.dot(x.T))))).T\n",
        "    #print('x_feat shape ', x_feat.shape)  \n",
        "    del x\n",
        "    \n",
        "    y = y * normalizer\n",
        "    y_feat = (1*np.sqrt(1/(random_feats_lambda.shape[0])) *\\\n",
        "                      np.vstack((np.sin(lambda_*random_feats_lambda.dot(y.T)), \\\n",
        "                                np.cos(lambda_*random_feats_lambda.dot(y.T))))).T\n",
        "    \n",
        "    del y\n",
        "    #print('y_feat shape ', y_feat.shape)    \n",
        "  \n",
        "  \n",
        "    return np.dot(x_feat, y_feat.T)\n",
        "\n",
        "def ang_hyb_att(x, y, random_feats_sfm, random_feats_lambda, normalize=False):\n",
        "\n",
        "    approx_softmax_trig_hyb = trig_att(x, y, random_feats_sfm, normalize=False)\n",
        "            \n",
        "    approx_softmax_pos_hyb = pos_att(x, y, random_feats_sfm, normalize=False)\n",
        "    \n",
        "    approx_softmax_ang = ang_hyb_lambda(x, y, random_feats_lambda, normalize=False)\n",
        "\n",
        "    approx_softmax_hyb_ang = np.multiply((approx_softmax_ang), approx_softmax_pos_hyb) + \\\n",
        "                            np.multiply((1 - approx_softmax_ang), approx_softmax_trig_hyb)\n",
        "\n",
        "    del approx_softmax_trig_hyb, approx_softmax_pos_hyb, approx_softmax_ang\n",
        "    return approx_softmax_hyb_ang\n",
        "\n",
        "def gau_hyb_att(x, y, random_feats_sfm, random_feats_lambda, normalize=False):\n",
        "\n",
        "    approx_softmax_trig_hyb = trig_att(x, y, random_feats_sfm, normalize=False)\n",
        "            \n",
        "    approx_softmax_pos_hyb =pos_att(x, y, random_feats_sfm, normalize=False)\n",
        "\n",
        "    approx_softmax_gau = gau_hyb_lambda(x, y, random_feats_lambda, normalize=False)\n",
        "            \n",
        "    approx_softmax_hyb_gau = np.multiply((1-approx_softmax_gau), approx_softmax_pos_hyb) + \\\n",
        "                            np.multiply(approx_softmax_gau, approx_softmax_trig_hyb)\n",
        "\n",
        "    del approx_softmax_trig_hyb, approx_softmax_pos_hyb, approx_softmax_gau\n",
        "    return approx_softmax_hyb_gau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGyH9JMLIddE"
      },
      "source": [
        "# Generate Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwHBHICFIjbm"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.2):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp) # Token2Embeddings\n",
        "       \n",
        "        self.rnn = nn.LSTM(ninp, ninp, nlayers, dropout=dropout)\n",
        "        \n",
        "        self.decoder = nn.Linear(nhid, ntoken, bias=False)\n",
        "\n",
        "        # Optionally tie weights as in:\n",
        "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "        # https://arxiv.org/abs/1608.05859\n",
        "        # and\n",
        "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
        "        # https://arxiv.org/abs/1611.01462\n",
        "    \n",
        "        self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        \n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.weight)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "       \n",
        "\n",
        "    def forward(self, input, hidden, softmax_temp = 1):\n",
        "\n",
        "        #self.encoder.weight = torch.nn.Parameter(self.encoder.weight / torch.norm(self.encoder.weight, dim=1, keepdim=True))\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        #emb = F.normalize(emb, p=2, dim=1)\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        #output = F.normalize(output.view(output.size(0)*output.size(1), output.size(2)), p=2, dim=1)\n",
        "        output = output.view(output.size(0)*output.size(1), output.size(2))\n",
        "        #self.decoder.weight = torch.nn.Parameter(self.decoder.weight / torch.norm(self.decoder.weight, dim=1, keepdim=True))\n",
        "\n",
        "        decoded = np.sqrt(softmax_temp) * self.decoder(np.sqrt(softmax_temp) * output)\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "       \n",
        "        return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
        "                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io42Ik0eMzG9"
      },
      "source": [
        "train_batch_size = 32\n",
        "eval_batch_size = 10\n",
        "\n",
        "data_path = '/content/drive/MyDrive/Softmax_sampling/data/ptb'\n",
        "corpus_raw = Corpus(data_path)\n",
        "\n",
        "train_data = batchify(corpus_raw.train, train_batch_size) # size(total_len//bsz, bsz)\n",
        "val_data = batchify(corpus_raw.valid, eval_batch_size)\n",
        "test_data = batchify(corpus_raw.test, eval_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s3KEBP1M21I"
      },
      "source": [
        "interval = 200 # interval to report\n",
        "ntokens = len(corpus_raw.dictionary)\n",
        "\n",
        "\n",
        "#model hyperparameters\n",
        "\n",
        "hidden_size = 200\n",
        "\n",
        "n_layers = 2\n",
        "net = RNNModel(ntokens, hidden_size, hidden_size, n_layers, dropout=.2)\n",
        "bptt = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiZAfNTDM5V6",
        "outputId": "852ad162-1923-4a8d-be5d-382c7bdc993d"
      },
      "source": [
        "#Download from drive and put the right path\n",
        "net.load_state_dict(torch.load('/content/drive/MyDrive/Softmax_sampling/models/ptb/lstm_ptb_tied_warmstart_try4.pkl', map_location=torch.device('cpu')))\n",
        "net.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (drop): Dropout(p=0.2, inplace=False)\n",
              "  (encoder): Embedding(10000, 200)\n",
              "  (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)\n",
              "  (decoder): Linear(in_features=200, out_features=10000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l96Wm3ovM-1d"
      },
      "source": [
        "def get_model_embeddings(data_source, net, bptt):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    \"\"\"Computes the model embeddings. \n",
        "    Args: data_source = test dataloder\n",
        "          net = trained LSTM with weight tied\n",
        "          bptt = Batch size as defined in the args in main.py\n",
        "    \n",
        "    Output: Tensor of shape (data_source.reshape(-1), output of net) \n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "       \n",
        "        ntokens = len(corpus_raw.dictionary)\n",
        "        hidden = net.init_hidden(eval_batch_size) #hidden size(nlayers, bsz, hdsize)\n",
        "        model_out = []\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):# iterate over every timestep\n",
        "\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            \n",
        "            emb = net.encoder(data)\n",
        "            emb = F.normalize(emb, p=2, dim=1)\n",
        "            output, hidden = net.rnn(emb, hidden)\n",
        "            #model_out.append(F.normalize(output.reshape(-1, output.shape[-1]), p=2, dim=1))\n",
        "            model_out.append(output.reshape(-1, output.shape[-1]))\n",
        "\n",
        "            # model input and output\n",
        "            # inputdata size(bptt, bsz), and size(bptt, bsz, embsize) after embedding\n",
        "            # output size(bptt*bsz, ntoken)\n",
        "            \n",
        "    return torch.cat((model_out), dim=0)\n",
        "\n",
        "def get_class_embeddings(net):\n",
        "\n",
        "    \"\"\"Computes class embeddings. \n",
        "    Args: net = trained LSTM with weight tied. \n",
        "    Outputs: class embeddings. Tensor of shape (ntokens, output size of net)\n",
        "    \n",
        "    \"\"\"\n",
        "    classes = torch.tensor([list(corpus_raw.dictionary.word2idx.values())]).squeeze()\n",
        "    embeddings = net.encoder(classes.to(device))\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "def cross_entropy(X,y):\n",
        "  \n",
        "    \"\"\"\n",
        "    X is the output from fully connected layer (num_examples x num_classes)\n",
        "    y is labels (num_examples x 1)\n",
        "    Note that y is not a one-hot encoded vector. \n",
        "   \n",
        "    \"\"\"\n",
        "    m = y.shape[0]\n",
        "    log_likelihood = -np.log(X[range(m),y])\n",
        "\n",
        "    loss = np.sum(log_likelihood) / m\n",
        "    return loss\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        total_loss = 0\n",
        "        ntokens = len(corpus_raw.dictionary)\n",
        "        hidden = net.init_hidden(eval_batch_size) #hidden size(nlayers, bsz, hdsize)\n",
        "        for i in range(0, data_source.size(0) - 1, 64):# iterate over every timestep\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            output, hidden = net(data, hidden)\n",
        "            # model input and output\n",
        "            # inputdata size(bptt, bsz), and size(bptt, bsz, embsize) after embedding\n",
        "            # output size(bptt*bsz, ntoken)\n",
        "            total_loss += len(data) * criterion(output, targets).data\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        return total_loss / len(data_source)\n",
        "\n",
        "def cross_entropy(X,y):\n",
        "    \"\"\"\n",
        "    X is the output from fully connected layer (num_examples x num_classes)\n",
        "    y is labels (num_examples x 1)\n",
        "    Note that y is not a one-hot encoded vector. \n",
        "   \n",
        "    \"\"\"\n",
        "    m = y.shape[0]\n",
        "    log_likelihood = -np.log(X[range(m),y])\n",
        "\n",
        "    loss = np.sum(log_likelihood) / m\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIOrA7xrNACW"
      },
      "source": [
        "## Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCmLqVejM_i_"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_NKJnzgNFoA"
      },
      "source": [
        "## Get embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P24kX4xkNCLD"
      },
      "source": [
        "model_embeddings = get_model_embeddings(test_data, net, bptt)\n",
        "class_embeddings = get_class_embeddings(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLPf5f5IuJS3"
      },
      "source": [
        "softmax_temp = 1\n",
        "true_sfm = torch.exp(torch.matmul(softmax_temp*model_embeddings,\n",
        "                        class_embeddings.T)).cpu().detach().numpy()\n",
        "true_sfm = true_sfm/np.sum(true_sfm, axis =1, keepdims = True)           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQa8QgwoqXaF"
      },
      "source": [
        "true_sfm.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tikwUUOqFYgO"
      },
      "source": [
        "# Wasserstein"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLJwzr94Y5aP"
      },
      "source": [
        "from scipy.stats import entropy\n",
        "from scipy.stats import ks_2samp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Jx3Yhr6Y8H"
      },
      "source": [
        "all_rf_dim = [64, 128, 256, 512]\n",
        "num_samples = 100\n",
        "#softmax_temp = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7JkjDgtpo7s"
      },
      "source": [
        "### Favor+"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDXL3O64moIR"
      },
      "source": [
        "softmax_temp = 1\n",
        "runtimes = 10\n",
        "inter = 4121\n",
        "\n",
        "all_negs = []\n",
        "for runtime in range(runtimes):\n",
        "\n",
        "    negs = 0\n",
        "    np.random.seed(runtime)\n",
        "    for rf_dim in all_rf_dim:    \n",
        "        #rf_dim = 128\n",
        "      \n",
        "        sfm_app = []\n",
        "        orth = orthogonal_gau(rf_dim, model_embeddings.shape[1])\n",
        "\n",
        "        for i in range(int(model_embeddings.shape[0]/inter))):\n",
        "            sfm_app.append(pos_att(softmax_temp*model_embeddings.cpu().numpy()[i*inter : (i+1)*inter],\n",
        "                          class_embeddings.cpu().detach().numpy(), orth))\n",
        "        \n",
        "        sfm_app = sfm_app/sfm_app.sum(axis=1)[:, np.newaxis]\n",
        "        negs +=len(sfm_app[sfm_app<0])\n",
        "        \n",
        "    all_negs.append(negs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHPo3YQ1ql6J"
      },
      "source": [
        "### RFF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTVzolNpJ13O"
      },
      "source": [
        "softmax_temp = 1\n",
        "runtimes = 10\n",
        "inter = 4121\n",
        "\n",
        "all_negs = []\n",
        "for runtime in range(runtimes):\n",
        "\n",
        "    negs = 0\n",
        "    np.random.seed(runtime)\n",
        "    for rf_dim in all_rf_dim:    \n",
        "        #rf_dim = 128\n",
        "      \n",
        "        sfm_app = []\n",
        "        orth = orthogonal_gau(rf_dim, model_embeddings.shape[1])\n",
        "\n",
        "        for i in range(int(model_embeddings.shape[0]/inter))):\n",
        "            sfm_app.append(trig_att(softmax_temp*model_embeddings.cpu().numpy()[i*inter : (i+1)*inter],\n",
        "                          class_embeddings.cpu().detach().numpy(), orth))\n",
        "        \n",
        "        sfm_app = sfm_app/sfm_app.sum(axis=1)[:, np.newaxis]\n",
        "        negs +=len(sfm_app[sfm_app<0])\n",
        "        \n",
        "    all_negs.append(negs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlACoH5ErAAP"
      },
      "source": [
        "### Angular"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G00LkUa7pZyF"
      },
      "source": [
        "all_rf_dim = np.array([64, 128, 256, 512])\n",
        "rf_dim_lam = np.array([8]*4)\n",
        "rf_dim_base = all_rf_dim / rf_dim_lam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL_LuR-qBeTk"
      },
      "source": [
        "rf_dim_lam, rf_dim_base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFvD3im-PbY1"
      },
      "source": [
        "softmax_temp = 1\n",
        "runtimes = 10\n",
        "inter = 4121\n",
        "\n",
        "all_negs = []\n",
        "for runtime in range(runtimes):\n",
        "\n",
        "    negs = 0\n",
        "    np.random.seed(runtime)\n",
        "    for rf_dim in all_rf_dim:    \n",
        "        #rf_dim = 128\n",
        "      \n",
        "        sfm_app = []\n",
        "        orth_base = orthogonal_gau(rf_dim_base[k], model_embeddings.shape[1])\n",
        "        orth_lam = orthogonal_gau(rf_dim_lam[k], model_embeddings.shape[1])\n",
        "        \n",
        "        for i in range(int(model_embeddings.shape[0]/inter))):\n",
        "            sfm_app.append(ang_hyb_att(softmax_temp*model_embeddings.cpu().numpy()[i*inter : (i+1)*inter],\n",
        "                          class_embeddings.cpu().detach().numpy(), orth_base, orth_lam))\n",
        "        \n",
        "        sfm_app = sfm_app/sfm_app.sum(axis=1)[:, np.newaxis]\n",
        "        negs +=len(sfm_app[sfm_app<0])\n",
        "        \n",
        "    all_negs.append(negs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4yCksp_raT9"
      },
      "source": [
        "### Gaussian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuhasoKCxccL"
      },
      "source": [
        "softmax_temp = 1\n",
        "runtimes = 10\n",
        "inter = 4121\n",
        "\n",
        "all_negs = []\n",
        "for runtime in range(runtimes):\n",
        "\n",
        "    negs = 0\n",
        "    np.random.seed(runtime)\n",
        "    for rf_dim in all_rf_dim:    \n",
        "        #rf_dim = 128\n",
        "      \n",
        "        sfm_app = []\n",
        "        orth_base = orthogonal_gau(rf_dim_base[k], model_embeddings.shape[1])\n",
        "        orth_lam = orthogonal_gau(rf_dim_lam[k], model_embeddings.shape[1])\n",
        "\n",
        "        for i in range(int(model_embeddings.shape[0]/inter))):\n",
        "            sfm_app.append(gau_hyb_att(softmax_temp*model_embeddings.cpu().numpy()[i*inter : (i+1)*inter],\n",
        "                          class_embeddings.cpu().detach().numpy(), orth_base, orth_lam))\n",
        "        \n",
        "        sfm_app = sfm_app/sfm_app.sum(axis=1)[:, np.newaxis]\n",
        "        negs +=len(sfm_app[sfm_app<0])\n",
        "        \n",
        "    all_negs.append(negs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}