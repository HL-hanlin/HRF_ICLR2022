{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from data_utils import * \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.2, softmax_temp=6):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp) # Token2Embeddings\n",
    "       \n",
    "        self.rnn = nn.LSTM(ninp, ninp, nlayers, dropout=dropout)\n",
    "        \n",
    "        self.decoder = nn.Linear(nhid, ntoken, bias=False)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "    \n",
    "        self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        \n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "       \n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "      \n",
    "        emb = self.drop(self.encoder(input))\n",
    "  \n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        output_scaled = softmax_temp*output.view(output.size(0)*output.size(1), output.size(2))\n",
    "     \n",
    "        decoded = self.decoder(output_scaled)\n",
    "        return decoded, hidden, output\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "       \n",
    "        return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
    "                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda') if torch.device.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32\n",
    "eval_batch_size = 10\n",
    "\n",
    "corpus_raw = Corpus('/home/ec2-user/wikitext-2/')\n",
    "\n",
    "train_data = batchify(corpus_raw.train, train_batch_size) # size(total_len//bsz, bsz)\n",
    "val_data = batchify(corpus_raw.valid, eval_batch_size)\n",
    "test_data = batchify(corpus_raw.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 200 # interval to report\n",
    "ntokens = len(corpus_raw.dictionary)\n",
    "\n",
    "\n",
    "#model hyperparameters\n",
    "hidden_size = 650\n",
    "\n",
    "n_layers = 2\n",
    "net = RNNModel(ntokens, hidden_size, hidden_size, n_layers, dropout=.2)\n",
    "bptt = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .01\n",
    "weight_decay = .0001\n",
    "opt = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        total_loss = 0\n",
    "        ntokens = len(corpus_raw.dictionary)\n",
    "        hidden = net.init_hidden(eval_batch_size) #hidden size(nlayers, bsz, hdsize)\n",
    "        for i in range(0, data_source.size(0) - 1, 64):# iterate over every timestep\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            output, hidden,_ = net(data, hidden)\n",
    "            # model input and output\n",
    "            # inputdata size(bptt, bsz), and size(bptt, bsz, embsize) after embedding\n",
    "            # output size(bptt*bsz, ntoken)\n",
    "            total_loss += len(data) * criterion(output, targets).data\n",
    "            hidden = repackage_hidden(hidden)\n",
    "        return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n",
    "    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
    "    Args:\n",
    "        optimizer (:class:`~torch.optim.Optimizer`):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (:obj:`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (:obj:`int`):\n",
    "            The total number of training steps.\n",
    "        last_epoch (:obj:`int`, `optional`, defaults to -1):\n",
    "            The index of the last epoch when resuming training.\n",
    "    Return:\n",
    "        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "num_training_steps = len(test_data)//2\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=opt,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss():\n",
    "    weight = deepcopy(net.encoder.weight)\n",
    "    x = torch.norm(weight, dim=1, keepdim=True)\n",
    "    y = torch.ones_like(x)\n",
    "    \n",
    "    return ((x-y)**2).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_queries(queries):\n",
    "    \n",
    "    q_norm = torch.norm(queries, dim=1, keepdim=True)\n",
    "    y = torch.ones_like(q_norm)\n",
    "    \n",
    "    return ((q_norm-y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "lambda_key = 2 #regularizer to constrain key norms close to 1\n",
    "lambda_query = 2 #regularizer to constrain query norms close to 1\n",
    "\n",
    "def train():\n",
    "\n",
    "    net.train()\n",
    "    total_loss = 0\n",
    "    ce_total_loss = 0\n",
    "    start_time = time.time()\n",
    "   \n",
    "    hidden = net.init_hidden(train_batch_size)\n",
    "   \n",
    "    # train_data size(batchcnt, bsz)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, 64)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        \n",
    "   \n",
    "        output, hidden, query = net(data, hidden)\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "        full_loss = loss + lambda_query* normalize_queries(query) + lambda_key*l2_loss()\n",
    "      \n",
    "        opt.zero_grad()\n",
    "        full_loss.backward()\n",
    "       \n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += full_loss.item()\n",
    "        ce_total_loss += loss.item()\n",
    "\n",
    "        if batch % interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / interval\n",
    "            ce_loss = ce_total_loss / interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f} | learning rate {:5.4f}'.format(\n",
    "                epoch, batch, len(train_data) // 64,\n",
    "                elapsed * 1000 / interval, cur_loss, math.exp(ce_loss), opt.param_groups[0]['lr']))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = None\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "           \n",
    "            with open('/home/ec2-user/trained_models/wikitext2_reg_lstm.pkl', 'wb') as f:\n",
    "                torch.save(net.state_dict(), f)\n",
    "                \n",
    "            best_val_loss = val_loss\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6573, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105.35124943205062"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(4.6573)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
