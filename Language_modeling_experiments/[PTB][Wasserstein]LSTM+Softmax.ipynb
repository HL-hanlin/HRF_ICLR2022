{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[PTB][Wasserstein]LSTM+Softmax.ipynb","provenance":[{"file_id":"1S1P1QgVA2Zrd_jUOFskjVU9v-uBFGb_J","timestamp":1633308568327},{"file_id":"1s1-OCnT6PD-935pIu9iSlk0hCX3hu4bM","timestamp":1632513041275},{"file_id":"16OerXxe_kc5wMSrz0Bpzszigpu3oZ8VG","timestamp":1632447786928},{"file_id":"1jhmyoNW7fI8D0978NKSu2dIZCNF7ug8i","timestamp":1632065189621},{"file_id":"1aVfAlJ3YbvJmXRSU4dhHaA_PggKl5oOl","timestamp":1631464814566}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FBXFtMqqJHqP","executionInfo":{"status":"ok","timestamp":1633102131987,"user_tz":240,"elapsed":23091,"user":{"displayName":"Rain Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0ZRZoVSN12S4BDWp5u61a1xEDoM3900j-eei1Hw=s64","userId":"13707204345251524029"}},"outputId":"56394caf-0711-4876-d860-cc919fa8eed0"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import torch\n","os.chdir('/content/drive/MyDrive/Softmax_sampling/code')\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"CGDl3pbTOs7X"},"source":["# Utils"]},{"cell_type":"code","metadata":{"id":"7lQCS4XKIf77"},"source":["import os\n","import numpy as np\n","import time\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from tqdm.notebook import trange, tqdm\n","from torch.optim.lr_scheduler import LambdaLR\n","from copy import deepcopy\n","\n","import argparse\n","import time\n","import math\n","import torch\n","import torch.nn as nn\n","from data_utils import *\n","#import model\n","import easydict\n","import torch.nn.functional as F\n","import torch.optim as optim\n","#import Adam\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XxOt0VSyOuzK"},"source":["def gram_schmidt_columns(X):\n","    '''\n","    Using QR decomoposition to obtain orthogonal matrix.\n","    \n","    Parameters\n","    ----------\n","    X : matrix, dimension = m * d, where m <= d\n","        Random feature matrix with l2 normalized row.\n","    Returns\n","    -------\n","    Q : matrix, dimension = m * d, where m <= d\n","        Orthogonal random feature matrix with l2 normalized row.\n","    '''\n","    Q, R = np.linalg.qr(X)\n","    return Q\n","\n","def orthgonalize(V):\n","    '''\n","    Generate matrix with multiple orthogonal blocks\n","    Parameters\n","    ----------\n","    V : matrix, dimension = m * d, where m > d\n","        Random feature matrix with l2 normalized row.\n","    Returns\n","    -------\n","    V_ : TYPE\n","        Random feature matrix with l2 normalized row and multiple\n","        blocks.\n","    '''\n","    N = V.shape[0]\n","    d = V.shape[1]\n","    turns = int(N/d)\n","    remainder = N%d\n","\n","    if turns:\n","        V_ = np.zeros_like(V)\n","\n","        for i in range(turns):\n","            v = gram_schmidt_columns(V[i*d:(i+1)*d, :].T).T\n","            V_[i*d:(i+1)*d, :] = v\n","        if remainder != 0:\n","            V_[(i+1)*d:,:] = gram_schmidt_columns(V[(i+1)*d:,:].T).T\n","    else:\n","        V_ = gram_schmidt_columns(V.T).T\n","\n","    return V_\n","\n","def orthogonal_gau(dim_0, dim_1):\n","\n","    V = np.random.normal(0, 1, (dim_0, dim_1))\n","    norms = np.linalg.norm(V, axis = 1)[:, np.newaxis]\n","    V_orth = orthgonalize(V)\n","    \n","    return V_orth*norms\n","\n","def trig_att(x, y, random_feats_sfm, normalize=False):\n","    \n","    l, d = x.shape\n","  \n","    normalizer = 1 / (d ** 0.25) if normalize else 1\n","    \n","    x = x * normalizer\n","    y = y * normalizer\n","    \n","    x_feat = np.sqrt(1/(random_feats_sfm.shape[0])) *\\\n","                 np.exp(np.linalg.norm(x, axis = 1)**2/2)[:, np.newaxis] *\\\n","                 np.vstack((np.sin(random_feats_sfm.dot(x.T)), \\\n","                            np.cos(random_feats_sfm.dot(x.T)))).T\n","    #print('x_feat shape ', x_feat.shape)  \n","    y_feat = np.sqrt(1/(random_feats_sfm.shape[0])) *\\\n","                 np.exp(np.linalg.norm(y, axis = 1)**2/2)[:, np.newaxis] *\\\n","                 np.vstack((np.sin(random_feats_sfm.dot(y.T)), \\\n","                            np.cos(random_feats_sfm.dot(y.T)))).T\n","    #print('y_feat shape ', y_feat.shape)    \n","  \n","    return np.dot(x_feat, y_feat.T)\n","    \n","def pos_att(x, y, random_feats_sfm, normalize=False):\n","    \n","    l, d = x.shape\n","  \n","    normalizer = 1 / (d ** 0.25) if normalize else 1\n","    \n","    x = x * normalizer\n","   \n","    \n","    x_feat = np.sqrt(1/(2*random_feats_sfm.shape[0])) * \\\n","                    np.exp(-np.linalg.norm(x, axis = 1)**2/2)[:, np.newaxis] *\\\n","                    np.vstack((np.exp(random_feats_sfm.dot(x.T)), \\\n","                                np.exp(-random_feats_sfm.dot(x.T)))).T\n","    del x\n","    #print('x_feat shape ', x_feat.shape)  \n","    y = y * normalizer\n","    y_feat = np.sqrt(1/(2*random_feats_sfm.shape[0])) * \\\n","                    np.exp(-np.linalg.norm(y, axis = 1)**2/2)[:, np.newaxis] *\\\n","                    np.vstack((np.exp(random_feats_sfm.dot(y.T)), \\\n","                                np.exp(-random_feats_sfm.dot(y.T)))).T\n","    #print('y_feat shape ', y_feat.shape)    \n","    \n","    del y\n","    return np.dot(x_feat, y_feat.T)\n","\n","def ang_hyb_lambda(x, y, random_feats_lambda, normalize=False):\n","    \n","    l, d = x.shape\n","  \n","    normalizer = 1 / (d ** 0.25) if normalize else 1\n","    \n","    x = x * normalizer\n","    \n","    x_feat = np.hstack((np.repeat(np.sqrt(1/2), x.shape[0])[:, np.newaxis],\\\n","                                      (1j*np.sqrt(1/(2*random_feats_lambda.shape[0])) *\\\n","                                      np.sign(random_feats_lambda.dot(x.T))).T))\n","    #print('x_feat shape ', x_feat.shape)  \n","    del x \n","    \n","    y = y * normalizer\n","    y_feat = np.hstack((np.repeat(np.sqrt(1/2), y.shape[0])[:, np.newaxis],\\\n","                                      (1j*np.sqrt(1/(2*random_feats_lambda.shape[0])) *\\\n","                                      np.sign(random_feats_lambda.dot(y.T))).T))\n","    #print('y_feat shape ', y_feat.shape) \n","    del y\n","  \n","    return np.dot(x_feat, y_feat.T).real\n","\n","def gau_hyb_lambda(x, y, random_feats_lambda, lambda_=1, normalize=False):\n","    \n","    l, d = x.shape\n","  \n","    normalizer = 1 / (d ** 0.25) if normalize else 1\n","    \n","    x = x * normalizer\n","\n","    x_feat = (1*np.sqrt(1/(random_feats_lambda.shape[0])) *\\\n","                      np.vstack((np.sin(lambda_*random_feats_lambda.dot(x.T)), \\\n","                                np.cos(lambda_*random_feats_lambda.dot(x.T))))).T\n","    #print('x_feat shape ', x_feat.shape)  \n","    del x\n","    \n","    y = y * normalizer\n","    y_feat = (1*np.sqrt(1/(random_feats_lambda.shape[0])) *\\\n","                      np.vstack((np.sin(lambda_*random_feats_lambda.dot(y.T)), \\\n","                                np.cos(lambda_*random_feats_lambda.dot(y.T))))).T\n","    \n","    del y\n","    #print('y_feat shape ', y_feat.shape)    \n","  \n","  \n","    return np.dot(x_feat, y_feat.T)\n","\n","def ang_hyb_att(x, y, random_feats_sfm, random_feats_lambda, normalize=False):\n","\n","    approx_softmax_trig_hyb = trig_att(x, y, random_feats_sfm, normalize=False)\n","            \n","    approx_softmax_pos_hyb = pos_att(x, y, random_feats_sfm, normalize=False)\n","    \n","    approx_softmax_ang = ang_hyb_lambda(x, y, random_feats_lambda, normalize=False)\n","\n","    approx_softmax_hyb_ang = np.multiply((approx_softmax_ang), approx_softmax_pos_hyb) + \\\n","                            np.multiply((1 - approx_softmax_ang), approx_softmax_trig_hyb)\n","\n","    del approx_softmax_trig_hyb, approx_softmax_pos_hyb, approx_softmax_ang\n","    return approx_softmax_hyb_ang\n","\n","def gau_hyb_att(x, y, random_feats_sfm, random_feats_lambda, normalize=False):\n","\n","    approx_softmax_trig_hyb = trig_att(x, y, random_feats_sfm, normalize=False)\n","            \n","    approx_softmax_pos_hyb =pos_att(x, y, random_feats_sfm, normalize=False)\n","\n","    approx_softmax_gau = gau_hyb_lambda(x, y, random_feats_lambda, normalize=False)\n","            \n","    approx_softmax_hyb_gau = np.multiply((1-approx_softmax_gau), approx_softmax_pos_hyb) + \\\n","                            np.multiply(approx_softmax_gau, approx_softmax_trig_hyb)\n","\n","    del approx_softmax_trig_hyb, approx_softmax_pos_hyb, approx_softmax_gau\n","    return approx_softmax_hyb_gau"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kGyH9JMLIddE"},"source":["# Generate Embeddings"]},{"cell_type":"code","metadata":{"id":"lwHBHICFIjbm"},"source":["class RNNModel(nn.Module):\n","    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n","\n","    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.2):\n","        super(RNNModel, self).__init__()\n","        self.drop = nn.Dropout(dropout)\n","        self.encoder = nn.Embedding(ntoken, ninp) # Token2Embeddings\n","       \n","        self.rnn = nn.LSTM(ninp, ninp, nlayers, dropout=dropout)\n","        \n","        self.decoder = nn.Linear(nhid, ntoken, bias=False)\n","\n","        # Optionally tie weights as in:\n","        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n","        # https://arxiv.org/abs/1608.05859\n","        # and\n","        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n","        # https://arxiv.org/abs/1611.01462\n","    \n","        self.decoder.weight = self.encoder.weight\n","\n","        self.init_weights()\n","\n","        \n","        self.nhid = nhid\n","        self.nlayers = nlayers\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n","        nn.init.zeros_(self.decoder.weight)\n","        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n","       \n","\n","    def forward(self, input, hidden, softmax_temp = 1):\n","\n","        #self.encoder.weight = torch.nn.Parameter(self.encoder.weight / torch.norm(self.encoder.weight, dim=1, keepdim=True))\n","        emb = self.drop(self.encoder(input))\n","        #emb = F.normalize(emb, p=2, dim=1)\n","        output, hidden = self.rnn(emb, hidden)\n","        output = self.drop(output)\n","        #output = F.normalize(output.view(output.size(0)*output.size(1), output.size(2)), p=2, dim=1)\n","        output = output.view(output.size(0)*output.size(1), output.size(2))\n","        #self.decoder.weight = torch.nn.Parameter(self.decoder.weight / torch.norm(self.decoder.weight, dim=1, keepdim=True))\n","\n","        decoded = np.sqrt(softmax_temp) * self.decoder(np.sqrt(softmax_temp) * output)\n","        return decoded, hidden\n","\n","    def init_hidden(self, bsz):\n","        weight = next(self.parameters()).data\n","       \n","        return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n","                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"io42Ik0eMzG9"},"source":["train_batch_size = 32\n","eval_batch_size = 10\n","\n","data_path = '/content/drive/MyDrive/Softmax_sampling/data/ptb'\n","corpus_raw = Corpus(data_path)\n","\n","train_data = batchify(corpus_raw.train, train_batch_size) # size(total_len//bsz, bsz)\n","val_data = batchify(corpus_raw.valid, eval_batch_size)\n","test_data = batchify(corpus_raw.test, eval_batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2s3KEBP1M21I"},"source":["interval = 200 # interval to report\n","ntokens = len(corpus_raw.dictionary)\n","\n","\n","#model hyperparameters\n","\n","hidden_size = 200\n","\n","n_layers = 2\n","net = RNNModel(ntokens, hidden_size, hidden_size, n_layers, dropout=.2)\n","bptt = 64"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BiZAfNTDM5V6","executionInfo":{"status":"ok","timestamp":1633102145443,"user_tz":240,"elapsed":2190,"user":{"displayName":"Rain Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0ZRZoVSN12S4BDWp5u61a1xEDoM3900j-eei1Hw=s64","userId":"13707204345251524029"}},"outputId":"1eba0519-d20f-48de-ef63-cd2fa9d6be83"},"source":["#Download from drive and put the right path\n","net.load_state_dict(torch.load('/content/drive/MyDrive/Softmax_sampling/models/ptb/lstm_ptb_tied_warmstart_try4.pkl', map_location=torch.device('cpu')))\n","net.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RNNModel(\n","  (drop): Dropout(p=0.2, inplace=False)\n","  (encoder): Embedding(10000, 200)\n","  (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)\n","  (decoder): Linear(in_features=200, out_features=10000, bias=False)\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"l96Wm3ovM-1d"},"source":["def get_model_embeddings(data_source, net, bptt):\n","    # Turn on evaluation mode which disables dropout.\n","    \"\"\"Computes the model embeddings. \n","    Args: data_source = test dataloder\n","          net = trained LSTM with weight tied\n","          bptt = Batch size as defined in the args in main.py\n","    \n","    Output: Tensor of shape (data_source.reshape(-1), output of net) \n","    \"\"\"\n","    with torch.no_grad():\n","        net.eval()\n","       \n","        ntokens = len(corpus_raw.dictionary)\n","        hidden = net.init_hidden(eval_batch_size) #hidden size(nlayers, bsz, hdsize)\n","        model_out = []\n","        for i in range(0, data_source.size(0) - 1, bptt):# iterate over every timestep\n","\n","            data, targets = get_batch(data_source, i)\n","            data, targets = data.to(device), targets.to(device)\n","            \n","            emb = net.encoder(data)\n","            emb = F.normalize(emb, p=2, dim=1)\n","            output, hidden = net.rnn(emb, hidden)\n","            #model_out.append(F.normalize(output.reshape(-1, output.shape[-1]), p=2, dim=1))\n","            model_out.append(output.reshape(-1, output.shape[-1]))\n","\n","            # model input and output\n","            # inputdata size(bptt, bsz), and size(bptt, bsz, embsize) after embedding\n","            # output size(bptt*bsz, ntoken)\n","            \n","    return torch.cat((model_out), dim=0)\n","\n","def get_class_embeddings(net):\n","\n","    \"\"\"Computes class embeddings. \n","    Args: net = trained LSTM with weight tied. \n","    Outputs: class embeddings. Tensor of shape (ntokens, output size of net)\n","    \n","    \"\"\"\n","    classes = torch.tensor([list(corpus_raw.dictionary.word2idx.values())]).squeeze()\n","    embeddings = net.encoder(classes.to(device))\n","    \n","    return embeddings\n","\n","def cross_entropy(X,y):\n","  \n","    \"\"\"\n","    X is the output from fully connected layer (num_examples x num_classes)\n","    y is labels (num_examples x 1)\n","    Note that y is not a one-hot encoded vector. \n","   \n","    \"\"\"\n","    m = y.shape[0]\n","    log_likelihood = -np.log(X[range(m),y])\n","\n","    loss = np.sum(log_likelihood) / m\n","    return loss\n","\n","def evaluate(data_source):\n","    # Turn on evaluation mode which disables dropout.\n","    with torch.no_grad():\n","        net.eval()\n","        total_loss = 0\n","        ntokens = len(corpus_raw.dictionary)\n","        hidden = net.init_hidden(eval_batch_size) #hidden size(nlayers, bsz, hdsize)\n","        for i in range(0, data_source.size(0) - 1, 64):# iterate over every timestep\n","            data, targets = get_batch(data_source, i)\n","            data, targets = data.to(device), targets.to(device)\n","            output, hidden = net(data, hidden)\n","            # model input and output\n","            # inputdata size(bptt, bsz), and size(bptt, bsz, embsize) after embedding\n","            # output size(bptt*bsz, ntoken)\n","            total_loss += len(data) * criterion(output, targets).data\n","            hidden = repackage_hidden(hidden)\n","        return total_loss / len(data_source)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wIOrA7xrNACW"},"source":["## Evaluate model"]},{"cell_type":"code","metadata":{"id":"xCmLqVejM_i_"},"source":["criterion = nn.CrossEntropyLoss().to(device)\n","evaluate(test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_NKJnzgNFoA"},"source":["## Get embeddings"]},{"cell_type":"code","metadata":{"id":"P24kX4xkNCLD"},"source":["model_embeddings = get_model_embeddings(test_data, net, bptt)\n","class_embeddings = get_class_embeddings(net)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lLPf5f5IuJS3"},"source":["softmax_temp = 1\n","true_sfm = torch.exp(torch.matmul(softmax_temp*model_embeddings,\n","                        class_embeddings.T)).cpu().detach().numpy()\n","true_sfm = true_sfm/np.sum(true_sfm, axis =1, keepdims = True)           "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qVP5zl4yeSqA"},"source":["true_sfm.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tikwUUOqFYgO"},"source":["# Wasserstein"]},{"cell_type":"code","metadata":{"id":"IWC9aCXOxTdj"},"source":["from scipy.stats import wasserstein_distance"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_Jx3Yhr6Y8H"},"source":["all_rf_dim = [64, 128, 256, 512]\n","num_samples = 100\n","#softmax_temp = 4"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K7JkjDgtpo7s"},"source":["### Favor+"]},{"cell_type":"code","metadata":{"id":"xDXL3O64moIR"},"source":["softmax_temp = 1\n","runtimes = 10\n","inter = 4121\n","\n","for runtime in range(runtimes):\n","\n","    all_wass_dist = []\n","    np.random.seed(runtime)\n","    for rf_dim in all_rf_dim:    \n","        #rf_dim = 128\n","      \n","        sfm_app = []\n","        orth = orthogonal_gau(rf_dim, model_embeddings.shape[1])\n","\n","        for i in range(int(model_embeddings.shape[0]/inter)):\n","            sfm_app.append(pos_att(softmax_temp*model_embeddings.cpu().numpy()[i*inter : (i+1)*inter],\n","                          class_embeddings.cpu().detach().numpy(), orth))\n","        sfm_app = np.array(sfm_app)\n","        sfm_app = sfm_app.reshape(-1, sfm_app.shape[2])\n","\n","        sfm_app[sfm_app<0] = 0\n","        sfm_app = sfm_app/np.sum(sfm_app, axis = 1, keepdims = True)\n","        wass_dist = []\n","        for i in trange(sfm_app.shape[0]):\n","            wass_dist.append(wasserstein_distance(u_values=np.arange(10000),\n","                                                  v_values=np.arange(10000),\n","                                                  u_weights=true_sfm[i],\n","                                                  v_weights=sfm_app[i]))\n","        all_wass_dist.append(sum(wass_dist)/len(wass_dist))\n","    np.save(f'pos_att_result_{runtime}.npy', np.array(all_wass_dist))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DuQaE1rXX06p"},"source":["### RFF"]},{"cell_type":"code","metadata":{"id":"OTVzolNpJ13O"},"source":["softmax_temp = 1\n","runtimes = 10\n","inter = 4121\n","\n","for runtime in range(runtimes):\n","\n","    all_wass_dist = []  \n","    np.random.seed(runtime) \n","    for rf_dim in all_rf_dim: \n","      \n","      sfm_app = []\n","      orth = orthogonal_gau(rf_dim, model_embeddings.shape[1])\n","      \n","      for i in range(int(model_embeddings.shape[0]/inter)):\n","          sfm_app.append(trig_att(softmax_temp*model_embeddings.cpu().numpy()[i*inter : (i+1)*inter],\n","                        class_embeddings.cpu().detach().numpy(), orth))\n","      sfm_app = np.array(sfm_app)\n","      sfm_app = sfm_app.reshape(-1, sfm_app.shape[2])       \n","      sfm_app[sfm_app<0] = 0\n","      sfm_app = sfm_app/np.sum(sfm_app, axis = 1, keepdims = True)\n","\n","\n","      wass_dist = []\n","      for i in trange(sfm_app.shape[0]):\n","          wass_dist.append(wasserstein_distance(u_values=np.arange(10000),\n","                                                v_values=np.arange(10000),\n","                                                u_weights=true_sfm[i],\n","                                                v_weights=sfm_app[i]))\n","      all_wass_dist.append(sum(wass_dist)/len(wass_dist))\n","\n","    np.save(f'trig_att_result_{runtime}.npy', np.array(all_wass_dist))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V6kADlUwX6Ss"},"source":["### Angular"]},{"cell_type":"code","metadata":{"id":"G00LkUa7pZyF"},"source":["all_rf_dim = np.array([64, 128, 256, 512])\n","rf_dim_lam = np.array([8]*4)\n","rf_dim_base = all_rf_dim / rf_dim_lam"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VL_LuR-qBeTk","executionInfo":{"status":"ok","timestamp":1632964714533,"user_tz":240,"elapsed":9,"user":{"displayName":"Rain Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0ZRZoVSN12S4BDWp5u61a1xEDoM3900j-eei1Hw=s64","userId":"13707204345251524029"}},"outputId":"24197a84-b137-48e6-b3c1-066fafe09f47"},"source":["rf_dim_lam, rf_dim_base"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([8, 8, 8, 8]), array([ 8., 16., 32., 64.]))"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"uFvD3im-PbY1"},"source":["softmax_temp = 1\n","inter = 4121\n","runtimes = 10\n","\n","for runtime in range(1, runtimes):\n","\n","    all_wass_dist = [] \n","    np.random.seed(runtime) \n","\n","    for k in range(4):    \n","        #rf_dim = 128\n","        \n","        orth_base = orthogonal_gau(int(rf_dim_base[k]), model_embeddings.shape[1])\n","        orth_lam = orthogonal_gau(int(rf_dim_lam[k]), model_embeddings.shape[1])\n","        sfm_app = []\n","        for i in range(int(model_embeddings.shape[0]/inter)):\n","            sfm_app.append(ang_hyb_att(softmax_temp*model_embeddings.cpu().numpy()[i*inter : (i+1)*inter],\n","                          class_embeddings.cpu().detach().numpy(), orth_base, orth_lam))\n","        sfm_app = np.array(sfm_app)\n","        sfm_app = sfm_app.reshape(-1, sfm_app.shape[2])\n","        sfm_app[sfm_app<0] = 0\n","        sfm_app = sfm_app/np.sum(sfm_app, axis = 1, keepdims = True)\n","\n","        wass_dist = []\n","        for i in trange(sfm_app.shape[0]):\n","            wass_dist.append(wasserstein_distance(u_values=np.arange(10000),\n","                                                  v_values=np.arange(10000),\n","                                                  u_weights=true_sfm[i],\n","                                                  v_weights=sfm_app[i]))\n","        all_wass_dist.append(sum(wass_dist)/len(wass_dist))\n","    print(all_wass_dist)\n","    np.save(f'ang_hyb_att_result_{runtime}.npy', np.array(all_wass_dist)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uF1JDeS9X8Nl"},"source":["### Gaussian"]},{"cell_type":"code","metadata":{"id":"XuhasoKCxccL"},"source":["softmax_temp = 1\n","inter = 4121\n","all_wass_dist = []\n","runtimes = 10\n","\n","for runtime in range(runtimes):\n","\n","    all_wass_dist = [] \n","    np.random.seed(runtime) \n","    for k in range(4):    \n","        #rf_dim = 128\n","        \n","        orth_base = orthogonal_gau(int(rf_dim_base[k]), model_embeddings.shape[1])\n","        orth_lam = orthogonal_gau(int(rf_dim_lam[k]), model_embeddings.shape[1])\n","        sfm_app = []\n","        for i in range(int(model_embeddings.shape[0]/inter)):\n","            sfm_app.append(gau_hyb_att(softmax_temp*model_embeddings.cpu().numpy()[i*inter : (i+1)*inter],\n","                          class_embeddings.cpu().detach().numpy(), orth_base, orth_lam))\n","        sfm_app = np.array(sfm_app)\n","        sfm_app = sfm_app.reshape(-1, sfm_app.shape[2])\n","        sfm_app[sfm_app<0] = 0\n","        sfm_app = sfm_app/np.sum(sfm_app, axis = 1, keepdims = True)\n","\n","        wass_dist = []\n","        for i in trange(sfm_app.shape[0]):\n","            wass_dist.append(wasserstein_distance(u_values=np.arange(10000),\n","                                                  v_values=np.arange(10000),\n","                                                  u_weights=true_sfm[i],\n","                                                  v_weights=sfm_app[i]))\n","        all_wass_dist.append(sum(wass_dist)/len(wass_dist))\n","\n","    np.save(f'gau_hyb_att_Wassresult_{runtime}.npy', np.array(all_wass_dist))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ujyhcntyxdzg"},"source":["all_wass_dist"],"execution_count":null,"outputs":[]}]}